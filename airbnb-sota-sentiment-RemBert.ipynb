{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9077606,"sourceType":"datasetVersion","datasetId":5421040}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langdetect","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:43:54.620205Z","iopub.execute_input":"2024-08-01T12:43:54.620820Z","iopub.status.idle":"2024-08-01T12:44:11.913211Z","shell.execute_reply.started":"2024-08-01T12:43:54.620763Z","shell.execute_reply":"2024-08-01T12:44:11.912136Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\nBuilding wheels for collected packages: langdetect\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=6397480268ca91bb2bf0afdc01c24574b52253b8e8fb0ea5bd43519c9ac463b5\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built langdetect\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"}]},{"cell_type":"code","source":"## Imports\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport torch\n#from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, TrainingArguments, Trainer  \nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nfrom sklearn.preprocessing import OneHotEncoder\nimport re  \nimport string  \nfrom bs4 import BeautifulSoup  \nfrom nltk.corpus import stopwords  \nfrom langdetect import detect  \nfrom sklearn.utils import resample\nfrom tqdm import tqdm\n\nimport nltk  \nnltk.download('stopwords')  \n\n## Suppress all warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T13:09:01.124428Z","iopub.execute_input":"2024-08-01T13:09:01.124699Z","iopub.status.idle":"2024-08-01T13:09:01.132021Z","shell.execute_reply.started":"2024-08-01T13:09:01.124677Z","shell.execute_reply":"2024-08-01T13:09:01.131177Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"## Load listings data  (NYC)\nnyc_train = pd.read_csv('/kaggle/input/airbnb-cities-reviews/nyc_train.csv')  \n\n## Load reviews data  (london)\nlondon_train = pd.read_csv('/kaggle/input/airbnb-cities-reviews/london_train.csv')  \n\n## Concat the cities\ntrain_set = pd.concat([nyc_train, london_train], axis=0)\n\n## Display head\ntrain_set.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:44:38.502144Z","iopub.execute_input":"2024-08-01T12:44:38.502886Z","iopub.status.idle":"2024-08-01T12:45:26.018189Z","shell.execute_reply.started":"2024-08-01T12:44:38.502854Z","shell.execute_reply":"2024-08-01T12:45:26.017242Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           listing_id                                      listing_url  \\\n0             3283409             https://www.airbnb.com/rooms/3283409   \n1            53031902            https://www.airbnb.com/rooms/53031902   \n2  611089534312850631  https://www.airbnb.com/rooms/611089534312850631   \n\n        scrape_id last_scraped                                       name  \\\n0  20240705150938   2024-07-05              Beautiful Brooklyn Brownstone   \n1  20240705150938   2024-07-05          Urban Chic Riverview Corner Suite   \n2  20240705150938   2024-07-05  Cozy and stylish ground floor guest suite   \n\n                               neighborhood_overview    host_id  \\\n0  Bed-Stuy is a quiet neighborhood. Tree-lined s...   16593547   \n1  Vibrant neighborhood, with people having fun i...  206778021   \n2  Morris park is the SAFEST neighborhood in the ...   19418202   \n\n   number_of_reviews  number_of_reviews_ltm  number_of_reviews_l30d  ...  \\\n0                193                      9                       0  ...   \n1                 37                      1                       0  ...   \n2                115                     25                       0  ...   \n\n  calculated_host_listings_count reviews_per_month  city                  id  \\\n0                              1              1.61   nyc            53603884   \n1                             12              1.13   nyc  775447743059830615   \n2                              1              4.41   nyc  763077599351696920   \n\n         date  reviewer_id  reviewer_name  \\\n0  2015-11-10     44268857          Nancy   \n1  2022-12-05    427701377         Filipe   \n2  2022-11-18     38157718        Malvina   \n\n                                            comments  polarity  polarity_class  \n0  We had an amazing stay at Mary's and Josh's be...    0.9796             1.0  \n1                                   Thanks Vanessa!!    0.5399             1.0  \n2  J'ai passé une excellent séjour chez Yulia et ...    0.5719             1.0  \n\n[3 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>listing_id</th>\n      <th>listing_url</th>\n      <th>scrape_id</th>\n      <th>last_scraped</th>\n      <th>name</th>\n      <th>neighborhood_overview</th>\n      <th>host_id</th>\n      <th>number_of_reviews</th>\n      <th>number_of_reviews_ltm</th>\n      <th>number_of_reviews_l30d</th>\n      <th>...</th>\n      <th>calculated_host_listings_count</th>\n      <th>reviews_per_month</th>\n      <th>city</th>\n      <th>id</th>\n      <th>date</th>\n      <th>reviewer_id</th>\n      <th>reviewer_name</th>\n      <th>comments</th>\n      <th>polarity</th>\n      <th>polarity_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3283409</td>\n      <td>https://www.airbnb.com/rooms/3283409</td>\n      <td>20240705150938</td>\n      <td>2024-07-05</td>\n      <td>Beautiful Brooklyn Brownstone</td>\n      <td>Bed-Stuy is a quiet neighborhood. Tree-lined s...</td>\n      <td>16593547</td>\n      <td>193</td>\n      <td>9</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1.61</td>\n      <td>nyc</td>\n      <td>53603884</td>\n      <td>2015-11-10</td>\n      <td>44268857</td>\n      <td>Nancy</td>\n      <td>We had an amazing stay at Mary's and Josh's be...</td>\n      <td>0.9796</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53031902</td>\n      <td>https://www.airbnb.com/rooms/53031902</td>\n      <td>20240705150938</td>\n      <td>2024-07-05</td>\n      <td>Urban Chic Riverview Corner Suite</td>\n      <td>Vibrant neighborhood, with people having fun i...</td>\n      <td>206778021</td>\n      <td>37</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>12</td>\n      <td>1.13</td>\n      <td>nyc</td>\n      <td>775447743059830615</td>\n      <td>2022-12-05</td>\n      <td>427701377</td>\n      <td>Filipe</td>\n      <td>Thanks Vanessa!!</td>\n      <td>0.5399</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>611089534312850631</td>\n      <td>https://www.airbnb.com/rooms/611089534312850631</td>\n      <td>20240705150938</td>\n      <td>2024-07-05</td>\n      <td>Cozy and stylish ground floor guest suite</td>\n      <td>Morris park is the SAFEST neighborhood in the ...</td>\n      <td>19418202</td>\n      <td>115</td>\n      <td>25</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>4.41</td>\n      <td>nyc</td>\n      <td>763077599351696920</td>\n      <td>2022-11-18</td>\n      <td>38157718</td>\n      <td>Malvina</td>\n      <td>J'ai passé une excellent séjour chez Yulia et ...</td>\n      <td>0.5719</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 29 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Function to downsample the majority class and balance the data  \ndef balance_data(df, target_column):  \n    \"\"\"  \n    Downsample the majority class to balance the dataset.  \n      \n    Parameters:  \n    df (pd.DataFrame): DataFrame containing the data  \n    target_column (str): The name of the target column  \n      \n    Returns:  \n    pd.DataFrame: Balanced DataFrame  \n    \"\"\"  \n    ## Separate majority and minority classes  \n    majority_class = df[df[target_column] == df[target_column].value_counts().idxmax()]  \n    minority_class = df[df[target_column] != df[target_column].value_counts().idxmax()]  \n      \n    ## Downsample majority class  \n    majority_class_downsampled = resample(majority_class,  \n                                          replace=False,\n                                          n_samples=len(minority_class),\n                                          random_state=123) \n      \n    ## Combine minority class with downsampled majority class  \n    balanced_df = pd.concat([minority_class, majority_class_downsampled])  \n    return balanced_df \n\n## Balance the dataset  \nbalanced_train_set = balance_data(train_set, 'polarity_class')  \nprint(f\"POLARITY CLASS DISTRIBUTION: {balanced_train_set['polarity_class'].value_counts()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:45:26.020204Z","iopub.execute_input":"2024-08-01T12:45:26.020664Z","iopub.status.idle":"2024-08-01T12:45:27.018121Z","shell.execute_reply.started":"2024-08-01T12:45:26.020620Z","shell.execute_reply":"2024-08-01T12:45:27.017045Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"POLARITY CLASS DISTRIBUTION: polarity_class\n0.0    94141\n1.0    94141\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"## Function to preprocess Airbnb reviews  \ndef preprocess_reviews(reviews):  \n    \"\"\"  \n    Preprocess Airbnb reviews by removing HTML tags, stop words (excluding important negators),   \n    non-English reviews, and punctuations.  \n      \n    Parameters:  \n    reviews (pd.Series): Series containing the reviews  \n      \n    Returns:  \n    pd.Series: Cleaned reviews  \n    \"\"\"  \n    stop_words = set(stopwords.words('english'))  \n      \n    ## Remove negators from the stop words list to retain them in the text  \n    negators = {'not', 'no', 'nor', 'never', 'none', 'nothing', 'nowhere', 'neither', 'hardly', 'scarcely', 'barely', 'don’t', 'isn’t', 'wasn’t', 'shouldn’t', 'wouldn’t', 'couldn’t', 'won’t', 'can’t', 'don’t'}  \n    stop_words = stop_words - negators  \n      \n    def clean_review(review):  \n        ## Remove HTML tags  \n        review = BeautifulSoup(review, \"html.parser\").get_text()  \n          \n        ## Detect if the review is in English  \n        try:  \n            if detect(review) != 'en':  \n                return None  \n        except:  \n            return None  \n          \n        ## Remove punctuations  \n        review = re.sub(f\"[{string.punctuation}]\", \" \", review)  \n          \n        ## Convert to lowercase and remove stop words  \n        review = ' '.join([word.lower() for word in review.split() if word.lower() not in stop_words])  \n        return review\n    \n    ## Apply the cleaning function to all reviews  \n    cleaned_reviews = reviews.apply(clean_review)   \n      \n    return cleaned_reviews \n\n## Preprocess reviews  \nbalanced_train_set['cleaned_comments'] = preprocess_reviews(balanced_train_set['comments'])  \n\n## Drop rows where cleaned_comments is None (non-English reviews)  \nbalanced_train_set = balanced_train_set.dropna(subset=['cleaned_comments'])\nprint(f\"NEW DATA SIZE: {balanced_train_set.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T12:45:27.019517Z","iopub.execute_input":"2024-08-01T12:45:27.020143Z","iopub.status.idle":"2024-08-01T13:08:59.843682Z","shell.execute_reply.started":"2024-08-01T12:45:27.020113Z","shell.execute_reply":"2024-08-01T13:08:59.842694Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"NEW DATA SIZE: (117656, 30)\n","output_type":"stream"}]},{"cell_type":"code","source":"## Features\n## Split the dataset into training and validation sets\ntrain_df, val_df = train_test_split(balanced_train_set, test_size=0.2, random_state=42, stratify=balanced_train_set['polarity_class'].values)\n\n## One-Hot encoding (polarity_class)\none_hot_encoder = OneHotEncoder(sparse=False)\ntrain_df['polarity_class'] = list(one_hot_encoder.fit_transform(train_df[['polarity_class']]))\nval_df['polarity_class'] = list(one_hot_encoder.fit_transform(val_df[['polarity_class']]))\n\n## Convert the split dataframes to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\n## Print the number of samples in each dataset to verify the split\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T13:31:29.238253Z","iopub.execute_input":"2024-08-01T13:31:29.239125Z","iopub.status.idle":"2024-08-01T13:31:30.504432Z","shell.execute_reply.started":"2024-08-01T13:31:29.239093Z","shell.execute_reply":"2024-08-01T13:31:30.503540Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of training samples: 94124\nNumber of validation samples: 23532\n","output_type":"stream"}]},{"cell_type":"code","source":"## Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained('google/rembert')\n\nmax_length = 256\n\n## Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['cleaned_comments'], padding='max_length', truncation=True, max_length=max_length)\n\n## Tokenize the training set\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\n## Rename the target column to 'labels'  \ntrain_dataset = train_dataset.rename_column(\"polarity_class\", \"labels\") \ntrain_dataset = train_dataset.remove_columns(['cleaned_comments'])\n\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n## Tokenize the validation set\nval_dataset = val_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.rename_column(\"polarity_class\", \"labels\") \nval_dataset = val_dataset.remove_columns(['cleaned_comments'])\n\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"execution":{"iopub.status.busy":"2024-08-01T13:31:42.511210Z","iopub.execute_input":"2024-08-01T13:31:42.511588Z","iopub.status.idle":"2024-08-01T13:32:12.019829Z","shell.execute_reply.started":"2024-08-01T13:31:42.511555Z","shell.execute_reply":"2024-08-01T13:32:12.018815Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/263 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f513c0afd84b25906e14d1ee054913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5922f720732947989cf93f3c2b2ac3c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/8.71M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a060cea5a244f139ff7fa996db0b15e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadf922fc5df4d6388d8093eedc06afc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/94124 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713985d7d539498caebed21e6faf9df2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad23dd394c64487a0241d68e5218bb5"}},"metadata":{}}]},{"cell_type":"code","source":"## Load the pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained('google/rembert', num_labels=balanced_train_set['polarity_class'].nunique())\n\n## Freeze all layers except the classifier  \nfor name, param in model.named_parameters():  \n    if 'classifier' not in name:  \n        param.requires_grad = False \n        \n## Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=5,\n    per_device_train_batch_size=256,\n    per_device_eval_batch_size=256,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy='epoch',\n)\n\n## Define the compute_metrics function  \ndef compute_metrics(p):  \n    pred_labels = p.predictions.argmax(-1)  \n    accuracy = accuracy_score(p.label_ids, pred_labels)  \n    report = classification_report(p.label_ids, pred_labels, output_dict=True)  \n    return {  \n        'accuracy': accuracy,  \n        'report': report  \n    }\n\n## Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n## Train the model\ntrainer.train()\n\n## Evaluate the model\nresults = trainer.evaluate()\n\n## Print the results\nprint(f\"Accuracy: {results['eval_accuracy']}\")\nprint(f\"Classification Report: {results['eval_report']}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T14:15:35.889954Z","iopub.execute_input":"2024-08-01T14:15:35.890621Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of RemBertForSequenceClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='106' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [106/920 38:29 < 5:01:15, 0.05 it/s, Epoch 0.57/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}