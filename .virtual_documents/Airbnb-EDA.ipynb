


## IMPORTING NECESSARY PACKAGES
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from sklearn.impute import SimpleImputer
from wordcloud import WordCloud

## Suppress all warnings
warnings.filterwarnings('ignore')

## Set visualization style
sns.set(style="whitegrid")





## Load listings data  (NYC)
nyc_train = pd.read_csv(r'H:\Queens\Data Analystics\Project\Dataset\nyc_train.csv')  

## Load reviews data  (london)
london_train = pd.read_csv(r'H:\Queens\Data Analystics\Project\Dataset\london_train.csv')  


## Concat the cities
train_set = pd.concat([nyc_train, london_train], axis=0)





## DATA VIEW 
print('NUMBER OF RECORDS: ', train_set.shape[0])
print('NUMBER OF FEATURES:', train_set.shape[1])
print('FEATURE LIST:', train_set.columns.to_list())





## View Basic Information about the Dataset
print(train_set.info())


## Summary statistics of categorical columns
train_set.describe(include=['object'])


## Check for missing values  
print("Missing Values in the Training Set")  
listings_nan_counts = train_set.isnull().sum()
print(listings_nan_counts[listings_nan_counts > 0])  


## Ways of addressing the nan values
# 1. 'name': Fill missing values with the most frequent value (mode)
train_set['name'].fillna(train_set['name'].mode()[0], inplace=True)

# 2. 'neighborhood_overview': Dropping this column due to a large number of missing values 
train_set.drop(columns=['neighborhood_overview'], axis=1, inplace=True)

# For review score columns, we can use the mean or median for imputation
review_score_columns = ['review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin',
                        'review_scores_communication', 'review_scores_location', 'review_scores_value']

## Imputer instance for imputing the numerical values using the mean value
mean_imputer = SimpleImputer(strategy='mean')

# Apply mean imputation to the review score columns
train_set[review_score_columns] = mean_imputer.fit_transform(train_set[review_score_columns])

# 3. 'reviewer_name': Fill missing values with the most frequent value (mode)
train_set['reviewer_name'].fillna(train_set['reviewer_name'].mode()[0], inplace=True)


## Rechecking for missing values  
print("Missing Values after imputation:")  
listings_nan_counts = train_set.isnull().sum()
print(listings_nan_counts[listings_nan_counts > 0])  





## Distribution of review scores
review_columns = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 
                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 
                  'review_scores_value']

## Create a figure for plotting
plt.figure(figsize=(12, 18))

## Iterate over each review type to create subplots
for i, column in enumerate(review_columns, 1):
    plt.subplot(len(review_columns), 2, i*2-1)
    sns.histplot(train_set[train_set['city']=='nyc'][column], bins=20, kde=True, color='blue')
    plt.title(f'Distribution of {column} in NYC')
    plt.xlabel(column)
    plt.ylabel('Count')

    plt.subplot(len(review_columns), 2, i*2)
    sns.histplot(train_set[train_set['city']=='london'][column], bins=20, kde=True, color='orange')
    plt.title(f'Distribution of {column} in London')
    plt.xlabel(column)
    plt.ylabel('Count')

## Show plot
plt.tight_layout(pad=2.0)
plt.suptitle('Distribution of Review Scores by City', y=1.02, fontsize=16)
plt.show()








## Group by city and month, and count the number of listings
listings_over_time = train_set.groupby(['city', train_set['date'].dt.to_period('M')])['listing_id'].count().unstack(level=0)

## Convert PeriodIndex to Timestamp for plotting
listings_over_time.index = listings_over_time.index.to_timestamp()

## Plot number of listings over time for both cities
plt.figure(figsize=(14, 7))
listings_over_time.plot(kind='line', ax=plt.gca())
plt.title('Number of Listings Over Time by City')
plt.xlabel('Date')
plt.ylabel('Number of Listings')
plt.legend(title='City')
plt.show()








## Identify the top ten hosts based on the number of listings
top_hosts = train_set['name'].value_counts().nlargest(10).index

## Filter the dataset for the top ten hosts
top_hosts_listings = train_set[train_set['name'].isin(top_hosts)]

## Determine each city's contribution to the top ten hosts
city_contributions = top_hosts_listings.groupby(['name', 'city']).size().unstack(fill_value=0)

## Plotting the data
plt.figure(figsize=(14, 7))
city_contributions.plot(kind='bar', stacked=True, colormap='tab20c', ax=plt.gca())
plt.title('City Contributions to Top Ten Hosts')
plt.xlabel('City Code')
plt.ylabel('Number of Listings')
plt.xticks(rotation=90)
plt.legend(title='Host ID', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()








## Polarity class distribution
plt.figure(figsize=(10, 6))
ax = sns.countplot(data=train_set, x='polarity_class')
plt.title('Distribution of Polarity Classes')

## Add count and percentage labels
total = len(train_set['polarity_class'])
for container in ax.containers:
    counts = [int(v.get_height()) for v in container]
    percentages = [f'{100 * v.get_height() / total:.1f}%' for v in container]
    ax.bar_label(container, labels=percentages, label_type='center', fontsize=12, color='white')
    for i, count in enumerate(counts):
        ax.text(container[i].get_x() + container[i].get_width() / 2, container[i].get_height(), count,
                ha='center', va='bottom', fontsize=12, color='black')

plt.show()








# Filter data for New York City and London
nyc_data = train_set[train_set['city'] == 'nyc']
london_data = train_set[train_set['city'] == 'london']

# Separate positive and negative reviews for NYC
nyc_positive_comments = ' '.join(nyc_data[nyc_data['polarity_class'] == 1.0]['comments'].tolist())
nyc_negative_comments = ' '.join(nyc_data[nyc_data['polarity_class'] == 0.0]['comments'].tolist())

# Separate positive and negative reviews for London
london_positive_comments = ' '.join(london_data[london_data['polarity_class'] == 1.0]['comments'].tolist())
london_negative_comments = ' '.join(london_data[london_data['polarity_class'] == 0.0]['comments'].tolist())

# Generate word clouds
nyc_positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(nyc_positive_comments)
nyc_negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(nyc_negative_comments)
london_positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(london_positive_comments)
london_negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(london_negative_comments)

# Plot word clouds
plt.figure(figsize=(15, 14))

# NYC Positive word cloud
plt.subplot(2, 2, 1)
plt.imshow(nyc_positive_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('NYC Positive Reviews Word Cloud')

# NYC Negative word cloud
plt.subplot(2, 2, 2)
plt.imshow(nyc_negative_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('NYC Negative Reviews Word Cloud')

plt.show()

# Plot word clouds
plt.figure(figsize=(15, 14))

# London Positive word cloud
plt.subplot(2, 2, 1)
plt.imshow(london_positive_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('London Positive Reviews Word Cloud')

# London Negative word cloud
plt.subplot(2, 2, 2)
plt.imshow(london_negative_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('London Negative Reviews Word Cloud')

plt.show()








# Compute the number of words in each review
train_set['num_words'] = train_set['comments'].apply(lambda x: len(str(x).split()))

def plot_word_count_distribution(data, city_code, ax):
    # Filter data for the given city
    city_data = data[data['city'] == city_code]
    
    # Calculate statistics
    city_min = city_data['num_words'].min()
    city_max = city_data['num_words'].max()
    city_avg = city_data['num_words'].mean()
    
    # Plot distribution
    sns.histplot(city_data['num_words'], bins=30, kde=True, ax=ax)
    ax.axvline(city_min, color='red', linestyle='--', label=f'Min: {city_min}')
    ax.axvline(city_max, color='green', linestyle='--', label=f'Max: {city_max}')
    ax.axvline(city_avg, color='blue', linestyle='--', label=f'Avg: {city_avg:.2f}')
    ax.set_title(f'City {city_code} Review Word Count Distribution')
    ax.set_xlabel('Number of Words')
    ax.set_ylabel('Frequency')
    ax.legend()

# List of city codes
city_codes = train_set['city'].unique()

# Create subplots
fig, axes = plt.subplots(1, len(city_codes), figsize=(14, 6), sharey=True)

# Plot for each city
for ax, city_code in zip(axes, city_codes):
    plot_word_count_distribution(train_set, city_code, ax)

# Show the plots
plt.tight_layout()
plt.show()






